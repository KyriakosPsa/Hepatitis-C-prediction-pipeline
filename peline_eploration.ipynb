{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame shape: (204, 13) \n",
      "Features shape: (204, 12) \n",
      "Labels shape (204,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[32. ,  0. , 43.2, ..., 80. , 33.8, 75.7],\n",
       "       [45. ,  0. , 41.7, ..., 71. , 67.4, 70.3],\n",
       "       [55. ,  0. , 41.5, ..., 80. , 12.4, 69.9],\n",
       "       ...,\n",
       "       [64. ,  1. , 29. , ..., 66.7, 64.2, 82. ],\n",
       "       [46. ,  1. , 33. , ..., 52. , 50. , 71. ],\n",
       "       [59. ,  1. , 36. , ..., 67. , 34. , 68. ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df = pd.read_csv(\"Hepatitis_C.csv\")\n",
    "X = df.drop(columns='label').to_numpy()\n",
    "y = df.iloc[:,-1].to_numpy()\n",
    "print(\"Data frame shape:\",df.shape,\"\\nFeatures shape:\",X.shape,\"\\nLabels shape\",y.shape)\n",
    "df.head()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,[0,2,3,4,5,6,7,8,9,11]] = scaler.fit_transform(X[:,[0,2,3,4,5,6,7,8,9,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB() has 2 tunable parameters:\n",
      " {'priors': None, 'var_smoothing': 1e-09}\n",
      "\n",
      "KNeighborsClassifier() has 8 tunable parameters:\n",
      " {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "\n",
      "LinearDiscriminantAnalysis() has 7 tunable parameters:\n",
      " {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}\n",
      "\n",
      "SVC() has 15 tunable parameters:\n",
      " {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [gnb, knn, lda, svm]:\n",
    "    params = model.get_params()\n",
    "    print(f\"{model} has {len(params)} tunable parameters:\\n {params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifierNCV(BaseEstimator):\n",
    "  \"\"\"Class to tune the hyperparameters of a classifier using optuna and nested cross validation,\n",
    "  the specified inner cross validation folds are used for hyperparameter tuning and the specified outer folds are used to evaluate the model performance.\n",
    "  \"\"\"\n",
    "  def __init__(self,classifier_name, seed = 42, outer_folds= 5, inner_folds = 3, shuffle = True, trial_num = 10, verbose = True):\n",
    "    self.shuffle = shuffle\n",
    "    self.outer_folds = outer_folds\n",
    "    self.inner_folds = inner_folds\n",
    "    self.trial_num = trial_num\n",
    "    self.classifier_name = classifier_name\n",
    "    self.model = model\n",
    "    self.shuffle\n",
    "    self.verbose = verbose\n",
    "    # Set the random seed for generating the seed for each trial\n",
    "    np.random.seed(seed)\n",
    "    self.trials_seeds = np.random.randint(0, 1000, size = trial_num)\n",
    "    if self.verbose:\n",
    "      print(\"The random seeds for each trial are:\", self.trials_seeds,\"\\n\")\n",
    "\n",
    "  # Fit the selected classifier in 10 cross validation trials for hyperparameter tuning\n",
    "  def fit(self, X, y):\n",
    "    #Initializing Naive Bayes classifier to be used as a baseline model\n",
    "    baseline_model = GaussianNB()\n",
    "    # Initialize an array to store the best parameters for outer cross validation loop\n",
    "    parameters = []\n",
    "    #Initializing the dataframes to store the average scores per trial\n",
    "    scores_df = pd.DataFrame(columns = [\"F1\", \"Balanced Accuracy\", \"Precision\", \"MCC\", \"Recall\", \"ROC AUC\"], index=[f\"Trial {i + 1}\" for i in range(self.trial_num)])\n",
    "    baseline_df = pd.DataFrame(columns = [\"F1\", \"Balanced Accuracy\", \"Precision\", \"MCC\", \"Recall\", \"ROC AUC\"], index=[f\"Trial {i + 1}\" for i in range(self.trial_num)])\n",
    "  \n",
    "    ############################# Nested cross validation for hyperparameter tuning ###################################################\n",
    "    #objective function to be optimized by optuna for each inner cross validation loop\n",
    "    # This function is called inside the for loop of the outer cross validation loop\n",
    "    def objective(trial):\n",
    "      #Knearst neighbors tuning\n",
    "      if self.classifier_name == \"KNeighborsclassifier\":\n",
    "        n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 10) \n",
    "        weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]) \n",
    "        p = trial.suggest_int(\"p\", 1, 2)\n",
    "        self.model = KNeighborsClassifier(n_neighbors = n_neighbors, weights = weights, p = p)   \n",
    "      #LDA tuning\n",
    "      if self.classifier_name == \"LinearDiscriminantAnalysis\":\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"svd\", \"lsqr\", \"eigen\"])\n",
    "        store_covariance = trial.suggest_categorical(\"store_covariance\", [True, False])\n",
    "        self.model = LinearDiscriminantAnalysis(solver = solver, store_covariance= store_covariance)\n",
    "      # Logistic regression tuning\n",
    "      if self.classifier_name == \"LogisticRegression\":\n",
    "        C = trial.suggest_float(\"C\", 0, 10)\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "        self.model = LogisticRegression(C = C, penalty = penalty, solver = 'liblinear')\n",
    "      #SVM tuning\n",
    "      if self.classifier_name == \"SVC\":\n",
    "        kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"])\n",
    "        C = trial.suggest_float(\"C\", 0, 10)\n",
    "        self.model = SVC(C = C, kernel= kernel)\n",
    "      #Fit the  model and predict on the outer test sets after hyperparameter tuning in the inner cross validation loop\n",
    "      cost_function = make_scorer(cohen_kappa_score)\n",
    "      scores = cross_val_score(self.model, X_train, y_train, cv = self.inner_cv , scoring= cost_function)\n",
    "      return scores.mean()\n",
    "    \n",
    "    ##################### Outer cross validation loop to evade overfitting on a single train/test split ############################\n",
    "    # Number of of outer cross validation loops deterimined by the number of seeds for the trials\n",
    "    trial_counter = 0\n",
    "    for seed in self.trials_seeds:\n",
    "      trial_scores = np.zeros((self.outer_folds, 6))\n",
    "      base_trial_scores = np.zeros((self.outer_folds, 6))\n",
    "      outer_cv_counter = 0\n",
    "      # Create the stratified fold objects for the outer and inner cross validation loops with different random states per trial\n",
    "      self.outer_cv = StratifiedKFold(n_splits = self.outer_folds, shuffle = self.shuffle, random_state= seed)\n",
    "      self.inner_cv = StratifiedKFold(n_splits = self.inner_folds, shuffle = self.shuffle, random_state= seed)\n",
    "\n",
    "      for train_index, test_index in self.outer_cv.split(X, y):\n",
    "        #Outer cross validation loop to evade overfitting on a single train/test split\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # inner cross validation loop to tune hyperparameterss\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=50)\n",
    "        params = study.best_params\n",
    "      \n",
    "        #Set the best hyperparameters for the model\n",
    "        self.model.set_params(**params)\n",
    "        parameters.append(params)\n",
    "\n",
    "        #Fit the model and predict on the outer test sets after hyperparameter tuning in the inner cross validation loop\n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        #Fit baseline classifier_name and predict on the outer test sets\n",
    "        baseline_model.fit(X_train, y_train)\n",
    "        y_base_pred = baseline_model.predict(X_test)\n",
    "\n",
    "        #Calculate scores for each outer cross validation loop and trial\n",
    "        trial_scores[outer_cv_counter,:] = self.score(y_test, y_pred)\n",
    "        base_trial_scores[outer_cv_counter,:] = self.score(y_test, y_base_pred)\n",
    "        # Move to the next outer cross validation fold\n",
    "        outer_cv_counter += 1       \n",
    "      #Increase the trial counter and go to the next trial\n",
    "      scores_df.loc[f\"Trial {trial_counter + 1}\"] = trial_scores.mean(axis=0)\n",
    "      baseline_df.loc[f\"Trial {trial_counter + 1}\"] = base_trial_scores.mean(axis=0)\n",
    "      trial_counter += 1\n",
    "    \n",
    "    ######################### Results section ########################################\n",
    "    if self.verbose:\n",
    "      mean_var_df = pd.concat([scores_df.mean(axis=0),scores_df.var(axis=0)], axis=1).rename(columns={0:\"Mean\", 1:\"Variance\"})\n",
    "      base_mean_var_df = pd.concat([baseline_df.mean(axis=0), baseline_df.var(axis=0)], axis=1).rename(columns={0:\"Mean\", 1:\"Variance\"})\n",
    "      print(f\"The average scores and their variance for {self.classifier_name} across all {self.trial_num} trials are:\")\n",
    "      print(mean_var_df,\"\\n\")\n",
    "      print (mean_var_df.mean(axis=0))\n",
    "      print(f\"The average Naive Bayes baseline scores and their variance across all {self.trial_num} trial are:\")\n",
    "      print(base_mean_var_df,\"\\n\")\n",
    "      print(base_mean_var_df.mean(axis=0))\n",
    "    return scores_df, baseline_df, parameters \n",
    "\n",
    "  def score(self,y_test, y_pred):\n",
    "    return roc_auc_score(y_test, y_pred), matthews_corrcoef(y_test, y_pred), balanced_accuracy_score(y_test, y_pred),\\\n",
    "    f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seeds for each trial are: [102 435 860 270 106  71 700  20 614 121] \n",
      "\n",
      "The average scores and their variance for KNeighborsclassifier across all 10 trials are:\n",
      "                       Mean  Variance\n",
      "F1                 0.766834  0.000637\n",
      "Balanced Accuracy  0.589937  0.002311\n",
      "Precision          0.766834  0.000637\n",
      "MCC                0.686463  0.001339\n",
      "Recall             0.830384  0.001391\n",
      "ROC AUC            0.597582  0.002022 \n",
      "\n",
      "Mean        0.706339\n",
      "Variance    0.001389\n",
      "dtype: float64\n",
      "The average Naive Bayes baseline scores and their variance across all 10 trial are:\n",
      "                       Mean  Variance\n",
      "F1                 0.843637  0.000116\n",
      "Balanced Accuracy  0.717075  0.000545\n",
      "Precision          0.843637  0.000116\n",
      "MCC                0.796065  0.000220\n",
      "Recall             0.865219  0.000537\n",
      "ROC AUC            0.750659  0.000330 \n",
      "\n",
      "Mean        0.802715\n",
      "Variance    0.000311\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "knn_tuner = BinaryClassifierNCV(classifier_name = \"KNeighborsclassifier\")\n",
    "scores_df, baseline_scores_df, parameters = knn_tuner.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seeds for each trial are: [102 435 860 270 106  71 700  20 614 121] \n",
      "\n",
      "The average scores and their variance for LinearDiscriminantAnalysis across all 10 trials are:\n",
      "                       Mean  Variance\n",
      "F1                 0.815423  0.000252\n",
      "Balanced Accuracy  0.705657  0.000897\n",
      "Precision          0.815423  0.000252\n",
      "MCC                0.760076  0.000802\n",
      "Recall             0.941380  0.000317\n",
      "ROC AUC            0.651429  0.000877 \n",
      "\n",
      "Mean        0.781565\n",
      "Variance    0.000566\n",
      "dtype: float64\n",
      "The average Naive Bayes baseline scores and their variance across all 10 trial are:\n",
      "                       Mean  Variance\n",
      "F1                 0.843637  0.000116\n",
      "Balanced Accuracy  0.717075  0.000545\n",
      "Precision          0.843637  0.000116\n",
      "MCC                0.796065  0.000220\n",
      "Recall             0.865219  0.000537\n",
      "ROC AUC            0.750659  0.000330 \n",
      "\n",
      "Mean        0.802715\n",
      "Variance    0.000311\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "lda_tuner = BinaryClassifierNCV(classifier_name = \"LinearDiscriminantAnalysis\")\n",
    "scores_df, baseline_scores_df, parameters = lda_tuner.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seeds for each trial are: [102 435 860 270 106  71 700  20 614 121] \n",
      "\n",
      "The average scores and their variance for LogisticRegression across all 10 trials are:\n",
      "                       Mean  Variance\n",
      "F1                 0.881040  0.000338\n",
      "Balanced Accuracy  0.789921  0.000899\n",
      "Precision          0.881040  0.000338\n",
      "MCC                0.847771  0.000536\n",
      "Recall             0.910432  0.000610\n",
      "ROC AUC            0.803297  0.001468 \n",
      "\n",
      "Mean        0.852250\n",
      "Variance    0.000698\n",
      "dtype: float64\n",
      "The average Naive Bayes baseline scores and their variance across all 10 trial are:\n",
      "                       Mean  Variance\n",
      "F1                 0.843637  0.000116\n",
      "Balanced Accuracy  0.717075  0.000545\n",
      "Precision          0.843637  0.000116\n",
      "MCC                0.796065  0.000220\n",
      "Recall             0.865219  0.000537\n",
      "ROC AUC            0.750659  0.000330 \n",
      "\n",
      "Mean        0.802715\n",
      "Variance    0.000311\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "lr_tuner = BinaryClassifierNCV(classifier_name = \"LogisticRegression\")\n",
    "scores_df,baseline_scores_df, parameters =  lr_tuner.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seeds for each trial are: [102 435 860 270 106  71 700  20 614 121] \n",
      "\n",
      "The average scores and their variance for SVC across all 10 trials are:\n",
      "                       Mean  Variance\n",
      "F1                 0.892429  0.000167\n",
      "Balanced Accuracy  0.804552  0.000607\n",
      "Precision          0.892429  0.000167\n",
      "MCC                0.860839  0.000341\n",
      "Recall             0.904973  0.000577\n",
      "ROC AUC            0.830440  0.000500\n",
      "The average Naive Bayes baseline scores and their variance across all 10 trial are:\n",
      "                       Mean  Variance\n",
      "F1                 0.843637  0.000116\n",
      "Balanced Accuracy  0.717075  0.000545\n",
      "Precision          0.843637  0.000116\n",
      "MCC                0.796065  0.000220\n",
      "Recall             0.865219  0.000537\n",
      "ROC AUC            0.750659  0.000330\n"
     ]
    }
   ],
   "source": [
    "svm_tuner = BinaryClassifierNCV(classifier_name = \"SVC\")\n",
    "scores_df,baseline_scores_df, parameters  = svm_tuner.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
